# 強化学習による迷路の探索

## 問題設定

迷路の自動探索を行うエージェントを $\epsilon$-Greedy TD(0) アルゴリズムにより訓練する．

- 壁方向には移動できないものとする．
- 現在の位置から移動できるマスを $(x', y')$ としたとき，瞬時報酬 $r(x', y')$ () は，$(x', y')$ がゴールならば 1, そうでないなら 0 とする．

## 手法

1. パラメータ $\epsilon, \gamma, \alpha$ を初期化
2. 状態価値関数を0に初期化
3. 以下の学習エピソードを繰り返す．「エピソード数が50以上500未満」かつ「状態価値関数の更新値が適当に定めた閾値よりも小さい」を停止条件として設定する．
   1. エージェントをスタート位置に置く
   2. $\epsilon$-Greedy$ 方策に基づき行動を選択
        - 確率 $1-\epsilon$ で $\argmax_{x', y'}(r(x', y') + \gamma V(x', y'))$ となる行動を選択し，確率 $\epsilon$ でランダムに行動を選択する．

3. 状態価値関数を以下の式に基づき更新する．
        - $V(x, y) := V(x, y) + \alpha(r + \gamma V(x', y') - V(x, y))$
4. $(x', y')$ がゴール出なければ，$(x, y) \leftarrow (x', y')$ として行動選択のステップまで戻る．

## 実装の詳細

- Agentクラス
  - 迷路の探索を行うAgentのクラス．
  - 自身の現在地と，今までの移動ログを保持する．

- TDクラス
  - Agentクラスのインスタンスと，ndarrayの迷路をフィールドとして保持し，探索アルゴリズムを実行・管理するクラス．
  - 迷路は txt 形式として読み込む．壁は#, ゴールはG, スタートはS, それ以外は半角スペースとして対応付けている．

## 実験結果
